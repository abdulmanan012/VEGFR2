{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSFbIMb87cHu"
      },
      "source": [
        "# **Bioinformatics VEGFR2 Project**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iQiERxumDor"
      },
      "source": [
        "## **ChEMBL Database**\n",
        "\n",
        "The [*ChEMBL Database*](https://www.ebi.ac.uk/chembl/) is a database that contains curated bioactivity data of more than 2.5 million compounds. It is compiled from more than 92,100 documents, 1.7 million assays and the data spans 16,000 targets and 2,100 cells and 48,800 indications.\n",
        "[Data of May 07, 2025; ChEMBL version 35]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryGAwAIQ4yf"
      },
      "source": [
        "## **Installing libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toGT1U_B7F2i"
      },
      "source": [
        "Install the ChEMBL web service package so that we can retrieve bioactivity data from the ChEMBL Database."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UCQUw0BgTEZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJGExHQBfLh7"
      },
      "outputs": [],
      "source": [
        "! pip install chembl_webresource_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0kJjL8gb5nX"
      },
      "source": [
        "## **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXoCvMPPfNrv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from chembl_webresource_client.new_client import new_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lBsDrD0gAqH"
      },
      "source": [
        "### **Target search for VEGFR2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxtp79so4ZjF"
      },
      "outputs": [],
      "source": [
        "# Target search for coronavirus\n",
        "target = new_client.target\n",
        "target_query = target.search('CHEMBL279')\n",
        "targets = pd.DataFrame.from_dict(target_query)\n",
        "targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5OPfEALjAfZ"
      },
      "source": [
        ":### **Select and retrieve bioactivity data for *VEGFR2***\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSQ3aroOgML7"
      },
      "source": [
        "We will assign the 1st entry (which corresponds to the target protein, *VEGFR2* to the ***selected_target*** variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StrcHMVLha7u"
      },
      "outputs": [],
      "source": [
        "selected_target = targets.target_chembl_id[0]\n",
        "selected_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWd2DRalgjzB"
      },
      "source": [
        "Here, we will retrieve only bioactivity data for *VEGFR2* (CHEMBL279) that are reported as pChEMBL values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeFbV_CsSP8D"
      },
      "outputs": [],
      "source": [
        "activity = new_client.activity\n",
        "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrzAcEFnYzBY"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9iUAXFdSkoM"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXMpFNUOn_8"
      },
      "source": [
        "## **Handling missing data**\n",
        "If any compounds has missing value for the **standard_value** and **canonical_smiles** column then drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVOdk6ZR396"
      },
      "outputs": [],
      "source": [
        "df2 = df[df.standard_value.notna()]\n",
        "df2 = df2[df.canonical_smiles.notna()]\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCkuaeVNbdtM"
      },
      "outputs": [],
      "source": [
        "len(df2.canonical_smiles.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbAK3K7wclkS"
      },
      "outputs": [],
      "source": [
        "df2_nr = df2.drop_duplicates(['canonical_smiles'])\n",
        "df2_nr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSYHTzIgDMpM"
      },
      "outputs": [],
      "source": [
        "print(type(df2_nr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF06g7s-DQBD"
      },
      "outputs": [],
      "source": [
        "df2_nr.to_csv('df2_nr.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H4sSFAWhV9B"
      },
      "source": [
        "## **Data pre-processing of the bioactivity data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv2dzid_hzKd"
      },
      "source": [
        "### **Combine the 3 columns (molecule_chembl_id,canonical_smiles,standard_value) and bioactivity_class into a DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NCLYmrASgha"
      },
      "outputs": [],
      "source": [
        "selection = ['molecule_chembl_id','canonical_smiles','standard_value']\n",
        "df3 = df2_nr[selection]\n",
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ObVWGmkj_L"
      },
      "source": [
        "Saves dataframe to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skuSjuDu3Vrm"
      },
      "outputs": [],
      "source": [
        "df3.to_csv('df3.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO22XVlzhkXR"
      },
      "source": [
        "### **Labeling compounds as either being active, or inactive**\n",
        "The bioactivity data is in the IC50 unit. Compounds having values of less than 100 nM will be considered to be **active** while those greater than 1,000 nM will be considered to be **inactive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck9Z2M9CkRy_"
      },
      "outputs": [],
      "source": [
        "df4 = pd.read_csv('df3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnfKPbctEmSA"
      },
      "outputs": [],
      "source": [
        "df4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPKqSz7tMbcY"
      },
      "outputs": [],
      "source": [
        "bioactivity_threshold = []\n",
        "for i in df4.standard_value:\n",
        "  if float(i) >= 100:\n",
        "    bioactivity_threshold.append(\"inactive\")\n",
        "  else float(i) <= 100:\n",
        "    bioactivity_threshold.append(\"active\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Li64nUiZQ-y2"
      },
      "outputs": [],
      "source": [
        "bioactivity_class = pd.Series(bioactivity_threshold, name='class')\n",
        "df5 = pd.concat([df4, bioactivity_class], axis=1)\n",
        "df5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tlgyexWh7YJ"
      },
      "source": [
        "Saves dataframe to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSNia7suXstR"
      },
      "outputs": [],
      "source": [
        "df5.to_csv('df5.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Y7_lgN4jzM"
      },
      "source": [
        "# **Exploratory Data Analysis**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-4IOizard4P"
      },
      "source": [
        "## **Install conda and rdkit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0mjQ2PcrSe5"
      },
      "outputs": [],
      "source": [
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "! conda install -c rdkit rdkit -y\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmxXXFa4wTNG"
      },
      "source": [
        "## **Load bioactivity data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AMm19NW0bJR"
      },
      "outputs": [],
      "source": [
        "df5_no_smiles = df4.drop(columns='canonical_smiles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpkitQUutGAF"
      },
      "outputs": [],
      "source": [
        "df4_no_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aymiQsfdr5sY"
      },
      "outputs": [],
      "source": [
        "smiles = []\n",
        "\n",
        "for i in df4.canonical_smiles.tolist():\n",
        "  cpd = str(i).split('.')\n",
        "  cpd_longest = max(cpd, key = len)\n",
        "  smiles.append(cpd_longest)\n",
        "\n",
        "smiles = pd.Series(smiles, name = 'canonical_smiles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7a8qW_U85ZK"
      },
      "outputs": [],
      "source": [
        "df5_clean_smiles = pd.concat([df4_no_smiles,smiles], axis=1)\n",
        "df5_clean_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4xAp-NBtQtS"
      },
      "outputs": [],
      "source": [
        "df1 = df5_clean_smiles.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6Bf0uOGtef6"
      },
      "outputs": [],
      "source": [
        "df1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzN_S4Quro5S"
      },
      "source": [
        "## **Calculate Lipinski descriptors**\n",
        "Christopher Lipinski, a scientist at Pfizer, came up with a set of rule-of-thumb for evaluating the **druglikeness** of compounds. Such druglikeness is based on the Absorption, Distribution, Metabolism and Excretion (ADME) that is also known as the pharmacokinetic profile. Lipinski analyzed all orally active FDA-approved drugs in the formulation of what is to be known as the **Rule-of-Five** or **Lipinski's Rule**.\n",
        "\n",
        "The Lipinski's Rule stated the following:\n",
        "* Molecular weight < 500 Dalton\n",
        "* Octanol-water partition coefficient (LogP) < 5\n",
        "* Hydrogen bond donors < 5\n",
        "* Hydrogen bond acceptors < 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qn_eQcnxY7C"
      },
      "source": [
        "### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kibIqztBFTVo"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit-pypi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgBjIdT-rnRU"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem import Descriptors, Lipinski\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsgTV-ByxdMa"
      },
      "source": [
        "### **Calculate descriptors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCXEY7a9ugO_"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def lipinski(smiles, verbose=False):\n",
        "    moldata = []\n",
        "    for elem in smiles:\n",
        "        mol = Chem.MolFromSmiles(elem)\n",
        "        moldata.append(mol)\n",
        "\n",
        "    baseData = np.arange(1, 1)\n",
        "    i = 0\n",
        "    for mol in moldata:\n",
        "        desc_MolWt = Descriptors.MolWt(mol)\n",
        "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
        "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
        "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
        "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
        "\n",
        "\n",
        "        # Calculate TPSA\n",
        "        desc_TPSA = Descriptors.TPSA(mol)\n",
        "\n",
        "        row = np.array([desc_MolWt,\n",
        "                        desc_MolLogP,\n",
        "                        desc_NumHDonors,\n",
        "                        desc_NumHAcceptors,\n",
        "                        desc_NumRotatableBonds,\n",
        "                        desc_TPSA  # Add TPSA to the array\n",
        "                        ])\n",
        "\n",
        "        if i == 0:\n",
        "            baseData = row\n",
        "        else:\n",
        "            baseData = np.vstack([baseData, row])\n",
        "        i += 1\n",
        "\n",
        "    columnNames = [\"MW\", \"LogP\", \"NumHDonors\", \"NumHAcceptors\", \"NumRotatableBonds\", \"TPSA\"]\n",
        "    descriptors = pd.DataFrame(data=baseData, columns=columnNames)\n",
        "\n",
        "    return descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThFIFw8IukMY"
      },
      "outputs": [],
      "source": [
        "df_lipinski = lipinski(df.canonical_smiles)\n",
        "df_lipinski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMlPfFrxicj"
      },
      "source": [
        "### **Combine DataFrames**\n",
        "\n",
        "Let's take a look at the 2 DataFrames that will be combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaezyM5vwp9n"
      },
      "outputs": [],
      "source": [
        "df_lipinski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eET6iZ1Aw3oe"
      },
      "source": [
        "Now, let's combine the 2 DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9nUZC0Ww3gp"
      },
      "outputs": [],
      "source": [
        "df_combined = pd.concat([df,df_lipinski], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRBfBP3QxFJp"
      },
      "outputs": [],
      "source": [
        "df_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sugHc8AjSiCW"
      },
      "outputs": [],
      "source": [
        "df_combined.to_csv('Lipinski.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0MLOedB6j96"
      },
      "source": [
        "### **Convert IC50 to pIC50**\n",
        "To allow **IC50** data to be more uniformly distributed, we will convert **IC50** to the negative logarithmic scale which is essentially **-log10(IC50)**.\n",
        "\n",
        "This custom function pIC50() will accept a DataFrame as input and will:\n",
        "* Take the IC50 values from the ``standard_value`` column and converts it from nM to M by multiplying the value by 10$^{-9}$\n",
        "* Take the molar value and apply -log10\n",
        "* Delete the ``standard_value`` column and create a new ``pIC50`` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXMuFQoQ4pZF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pIC50(input):\n",
        "    pIC50 = []\n",
        "\n",
        "    for i in input['standard_value_norm']:\n",
        "        molar = i*(10**-9) # Converts nM to M\n",
        "        pIC50.append(-np.log10(molar))\n",
        "\n",
        "    input['pIC50'] = pIC50\n",
        "    x = input.drop('standard_value_norm', axis=1)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU5Fh1h2OaJJ"
      },
      "source": [
        "Point to note: Values greater than 100,000,000 will be fixed at 100,000,000 otherwise the negative logarithmic value will become negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuUTFUpcR1wU"
      },
      "outputs": [],
      "source": [
        "df_combined.standard_value.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyiJ0to5N6Z_"
      },
      "outputs": [],
      "source": [
        "-np.log10( (10**-9)* 100000000 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S1aJkOYOP6K"
      },
      "outputs": [],
      "source": [
        "-np.log10( (10**-9)* 10000000000 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkrTs7RfPsrH"
      },
      "source": [
        "We will first apply the norm_value() function so that the values in the standard_value column is normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A81u0MSZtkhZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def norm_value(input):\n",
        "    norm = []\n",
        "\n",
        "    for i in input['standard_value']:\n",
        "        if i > 100000000:\n",
        "          i = 100000000\n",
        "        norm.append(i)\n",
        "\n",
        "    input['standard_value_norm'] = norm\n",
        "    x = input.drop('standard_value', axis=1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeSy_NVAsJr0"
      },
      "outputs": [],
      "source": [
        "df_norm = norm_value(df_combined)\n",
        "df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeCd9LPGI0vh"
      },
      "outputs": [],
      "source": [
        "df_norm.standard_value_norm.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX2Mj2-ZP1Rj"
      },
      "outputs": [],
      "source": [
        "df_final = pIC50(df_norm)\n",
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8lPVlz8ISPj"
      },
      "outputs": [],
      "source": [
        "df_final.to_csv('VEGFR2_pIC50.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDKZzmK57YnS"
      },
      "outputs": [],
      "source": [
        "df_final.pIC50.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0vqbQWfxsZu"
      },
      "source": [
        "## **Exploratory Data Analysis (Chemical Space Analysis) via Lipinski descriptors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18heJagiyHoF"
      },
      "source": [
        "### **Import library**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "k4ZBZ52vWlX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiarmFbOdG3H"
      },
      "source": [
        "### **Frequency plot of the 2 bioactivity classes**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final=pd.read_csv('/content/df_final_2classes.csv')"
      ],
      "metadata": {
        "id": "EdstY4MBclOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "metadata": {
        "id": "C-Mqq1bP4u9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set(style='ticks')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Make sure pandas is imported if not already\n",
        "\n",
        "# Load the dataframe (assuming this was done in a previous cell)\n",
        "# If not, uncomment the line below:\n",
        "# df_final=pd.read_csv('chembl_zbi_classification.csv')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Assuming 'bioactivity_class' is the correct column name based on previous steps\n",
        "sns.countplot(x='bioactivity_class', data=df_final, hue='bioactivity_class', edgecolor='black', palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Save the plot as an SVG file\n",
        "plt.savefig('plot_bioactivity_class.svg')\n",
        "\n",
        "# Optionally, you can display the plot as well\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "MB-RiVCTaTqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB68NKVG0j68"
      },
      "source": [
        "### **Scatter plot of MW versus LogP**\n",
        "\n",
        "It can be seen that the 2 bioactivity classes are spanning similar chemical spaces as evident by the scatter plot of MW vs LogP."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Ensure numpy is imported for np.nan and np.ndarray\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='MW', y='LogP', data=df_final, hue='bioactivity_class', size='pIC50', edgecolor='black', alpha=0.5, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('MW in g/mol', fontsize=14, fontweight='bold', fontstyle='normal')\n",
        "plt.ylabel('cLogP', fontsize=14, fontweight='bold', fontstyle='normal')\n",
        "plt.legend(bbox_to_anchor=(0.01, 1), loc=2, borderaxespad=0, fontsize=9)\n",
        "plt.savefig('plot_MW_vs_LogP.svg')"
      ],
      "metadata": {
        "id": "pDXAX4HvW24_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLAfyRwHyJfX"
      },
      "source": [
        "### **Box plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n1uIAivyOkY"
      },
      "source": [
        "#### **pIC50 value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDnMNIDKKyJy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'pIC50', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('pIC50 value', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_ic50.svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsOqKyysCZCv"
      },
      "source": [
        "**Statistical analysis | Mann-Whitney U Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKasIJ-xK_ge"
      },
      "outputs": [],
      "source": [
        "def mannwhitney(descriptor, verbose=False):\n",
        "  # https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/\n",
        "  from numpy.random import seed\n",
        "  from numpy.random import randn\n",
        "  from scipy.stats import mannwhitneyu\n",
        "\n",
        "# seed the random number generator\n",
        "  seed(1)\n",
        "\n",
        "# actives and inactives\n",
        "  selection = [descriptor, 'bioactivity_class']\n",
        "  df = df_final[selection]\n",
        "  active = df[df.bioactivity_class == 'active']\n",
        "  active = active[descriptor]\n",
        "\n",
        "  selection = [descriptor, 'bioactivity_class']\n",
        "  df = df_final[selection]\n",
        "  inactive = df[df.bioactivity_class == 'inactive']\n",
        "  inactive = inactive[descriptor]\n",
        "\n",
        "# compare samples\n",
        "  stat, p = mannwhitneyu(active, inactive)\n",
        "  #print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "# interpret\n",
        "  alpha = 0.05\n",
        "  if p > alpha:\n",
        "    interpretation = 'Same distribution (fail to reject H0)'\n",
        "  else:\n",
        "    interpretation = 'Different distribution (reject H0)'\n",
        "\n",
        "  results = pd.DataFrame({'Descriptor':descriptor,\n",
        "                          'Statistics':stat,\n",
        "                          'p':p,\n",
        "                          'alpha':alpha,\n",
        "                          'Interpretation':interpretation}, index=[0])\n",
        "  filename = 'mannwhitneyu_' + descriptor + '.csv'\n",
        "  results.to_csv(filename)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZmUgOmdYVm5"
      },
      "outputs": [],
      "source": [
        "mannwhitney('pIC50')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2UlCwPmyTBq"
      },
      "source": [
        "#### **MW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNlEEsDEx3m6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'MW', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('MW in g/mol', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_MW.svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRl2FvgHYqaG"
      },
      "outputs": [],
      "source": [
        "mannwhitney('MW')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5hyBhGqyc6J"
      },
      "source": [
        "#### **LogP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liEtkpI4yX9t"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'LogP', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('cLogP', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_LogP.svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KgV5v_oFLXh"
      },
      "source": [
        "**Statistical analysis | Mann-Whitney U Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B61UsGMIFLuE"
      },
      "outputs": [],
      "source": [
        "mannwhitney('LogP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4db7LZLRym2k"
      },
      "source": [
        "#### **NumHDonors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iru1JPM1yg5A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'NumHDonors', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('nHD', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_NumHDonors.svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM5vZWanFe3c"
      },
      "source": [
        "**Statistical analysis | Mann-Whitney U Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS-rOqq7Fd1E"
      },
      "outputs": [],
      "source": [
        "mannwhitney('NumHDonors')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOYQ3QiSyu7-"
      },
      "source": [
        "#### **NumHAcceptors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCw6tgNCyxHf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'NumHAcceptors', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('nHA', fontsize=14, fontweight='bold')\n",
        "\n",
        "\n",
        "plt.savefig('plot_NumHAcceptors.svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEQoDZctFtGG"
      },
      "outputs": [],
      "source": [
        "mannwhitney('NumHAcceptors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZfRetty7S5n"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'TPSA', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('TPSA in Å²', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_TPSA.svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UqXpoEi7cbl"
      },
      "outputs": [],
      "source": [
        "mannwhitney('TPSA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrEziBWM9b7S"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'NumRotatableBonds', hue='bioactivity_class', data = df_final, palette=['#feb236', '#6b5b95'])\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('nRot', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_nRot.svg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9SeJ7il874Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkgqoB9n9y1p"
      },
      "outputs": [],
      "source": [
        "mannwhitney('NumRotatableBonds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHca-ttQxlmR"
      },
      "source": [
        "# **BDescriptor Calculation and Dataset Preparation**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abtr2CxPxlmS"
      },
      "source": [
        "## **Download PaDEL-Descriptor**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! apt-get update\n",
        "! apt-get install -y openjdk-8-jre-headless\n",
        "\n",
        "from padelpy import padeldescriptor"
      ],
      "metadata": {
        "id": "t1yclyloVuIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8bSbVsrJgN-"
      },
      "outputs": [],
      "source": [
        "! pip install padelpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8kg1y4PJpOe"
      },
      "outputs": [],
      "source": [
        "! wget https://github.com/dataprofessor/padel/raw/main/fingerprints_xml.zip\n",
        "! unzip fingerprints_xml.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJtm7PRDJ7ij"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "xml_files = glob.glob(\"*.xml\")\n",
        "xml_files.sort()\n",
        "xml_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3eqDEwGKAQb"
      },
      "outputs": [],
      "source": [
        "FP_list = ['AtomPairs2DCount',\n",
        " 'AtomPairs2D',\n",
        " 'EState',\n",
        " 'CDKextended',\n",
        " 'CDK',\n",
        " 'CDKgraphonly',\n",
        " 'KlekotaRothCount',\n",
        " 'KlekotaRoth',\n",
        " 'MACCS',\n",
        " 'PubChem',\n",
        " 'SubstructureCount',\n",
        " 'Substructure']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYEWYxGUKF20"
      },
      "outputs": [],
      "source": [
        "fp = dict(zip(FP_list, xml_files))\n",
        "fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDJkY43R-9F"
      },
      "outputs": [],
      "source": [
        "selection = ['smiles','ID']\n",
        "df3_selection = df3[selection]\n",
        "df3_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRSCoPVDSkf5"
      },
      "outputs": [],
      "source": [
        "! cat molecule.smi | head -5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlYaJ9pzUGjS"
      },
      "outputs": [],
      "source": [
        "! cat molecule.smi | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1lqSXJGew4v"
      },
      "outputs": [],
      "source": [
        "descriptors = pd.read_csv(fingerprint_output_file)\n",
        "descriptors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFW3Mk9zxlmT"
      },
      "source": [
        "## **Calculate fingerprint descriptors**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fingerprint = 'PubChem'\n",
        "\n",
        "fingerprint_output_file = ''.join([fingerprint,'.csv']) #Substructure.csv\n",
        "fingerprint_descriptortypes = fp[fingerprint]\n",
        "\n",
        "padeldescriptor(mol_dir='molecule.smi',\n",
        "                d_file=fingerprint_output_file, #'Substructure.csv'\n",
        "                #descriptortypes='PubchemFingerprinter.xml',\n",
        "                descriptortypes= fingerprint_descriptortypes,\n",
        "                detectaromaticity=True,\n",
        "                standardizenitro=True,\n",
        "                standardizetautomers=True,\n",
        "                threads=2,\n",
        "                removesalt=True,\n",
        "                log=True,\n",
        "                fingerprints=True)"
      ],
      "metadata": {
        "id": "t5l9EgGzd76P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz-ahGbVKHL9"
      },
      "outputs": [],
      "source": [
        "fp['PubChem']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsjnVFMMxlmU"
      },
      "source": [
        "### **Calculate PaDEL descriptors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxsLzDxKOUrN"
      },
      "outputs": [],
      "source": [
        "df3_X = pd.read_csv('descriptors_output.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3kiO0gNxlmU"
      },
      "source": [
        "## **Preparing the X and Y Data Matrices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30aa4WP4ZA8M"
      },
      "source": [
        "### **X data matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acl7m5CtB6Qf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/mordred_descriptors.csv')"
      ],
      "metadata": {
        "id": "AS19BQyU1b4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSyvs11MCB3A"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_7 = df.drop(columns=['compounds', 'MW', 'pIC50', 'Groups', 'LogP','NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', 'TPSA'])"
      ],
      "metadata": {
        "id": "sgXSpxTy47Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qQCpX097qf_"
      },
      "source": [
        "### **3.4. Remove high correlation columns and low variance features**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "if not isinstance(df_7, pd.DataFrame):\n",
        "    df_7 = pd.DataFrame(df_7)\n",
        "\n",
        "# Correlation filtering\n",
        "correlation_matrix = df_7.corr()\n",
        "high_corr_columns = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.90:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            high_corr_columns.add(colname)\n",
        "\n",
        "df_7_filtered = df_7.drop(columns=high_corr_columns)\n",
        "print(f\"Removed columns: {high_corr_columns}\")\n",
        "print(f\"Retained columns after correlation filtering: {df_7_filtered.columns.tolist()}\")\n",
        "\n",
        "# Variance thresholding\n",
        "selection = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
        "X = selection.fit_transform(df_7_filtered)\n",
        "retained_columns_variance = df_7_filtered.columns[selection.get_support()].tolist()\n",
        "print(f\"Retained columns after variance thresholding: {retained_columns_variance}\")\n",
        "print(f\"Shape of transformed data: {X.shape}\")"
      ],
      "metadata": {
        "id": "oQhvifx1hihU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.to_csv('X.csv', index=True)"
      ],
      "metadata": {
        "id": "Ht5PBJYyDFyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXMZvkeIgBJ5"
      },
      "outputs": [],
      "source": [
        "Y=df['Groups']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjhOlkOVhSxR"
      },
      "source": [
        "## **4. Data split (80/20 ratio)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNmpglUeixWw"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, Y = shuffle(X, Y, random_state=42)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz1o3c1LhSxU"
      },
      "outputs": [],
      "source": [
        "X_train.shape, Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tnwDASChSxW"
      },
      "outputs": [],
      "source": [
        "X_test.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFpLoNRHeRa6"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF1NGGUmtZL9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"n_neighbors\": [3, 5, 7, 9],\"weights\": [\"uniform\", \"distance\"],\"p\": [1, 2]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "for i in range(n_repeats):\n",
        "    print(f\"Running repetition {i+1}/{n_repeats} ...\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100 + i\n",
        "    )\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "\n",
        "    for params in grid:\n",
        "        knn = KNeighborsClassifier(**params)\n",
        "        knn.fit(X_train, Y_train)\n",
        "        y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = knn\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob),\n",
        "    })\n",
        "    best_params_list.append(best_params)\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "best_params_df = pd.DataFrame(best_params_list)\n",
        "print(summary)\n",
        "print(best_params_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw8zbfA5vov6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"C\": [0.01, 0.1, 1, 10],\"penalty\": [\"l2\"],\"solver\": [\"lbfgs\"]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "for i in range(n_repeats):\n",
        "    print(f\"Running repetition {i+1}/{n_repeats} ...\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100 + i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    for params in grid:\n",
        "        lr = LogisticRegression(**params, max_iter=1000)\n",
        "        lr.fit(X_train, Y_train)\n",
        "        y_prob = lr.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = lr\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob),\n",
        "    })\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "print(summary)\n",
        "print(best_params_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JYDMc-MwPXb"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "\n",
        "for i in range(n_repeats):\n",
        "    print(f\"Running repetition {i+1}/{n_repeats} ...\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100 + i\n",
        "    )\n",
        "\n",
        "    nb = GaussianNB()\n",
        "    nb.fit(X_train, Y_train)\n",
        "    y_pred = nb.predict(X_test)\n",
        "    y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "print(summary)\n",
        "print(best_params_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpf0x1czwsGM"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"C\": [0.1, 1, 10],\"kernel\": [\"linear\", \"rbf\"],\"gamma\": [\"scale\", \"auto\"]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "best_params_list = []\n",
        "\n",
        "for i in range(n_repeats):\n",
        "    print(f\"Running repetition {i+1}/{n_repeats} ...\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100 + i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "\n",
        "    for params in grid:\n",
        "        svc = SVC(probability=True, **params)\n",
        "        svc.fit(X_train, Y_train)\n",
        "        y_prob = svc.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = svc\n",
        "            best_params = params\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob),\n",
        "    })\n",
        "    best_params_list.append(best_params)\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "best_params_df = pd.DataFrame(best_params_list)\n",
        "\n",
        "print(summary)\n",
        "print(best_params_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv_Rh-VzFSeC"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"n_estimators\": [100, 200],\"max_depth\": [5, 10, -1],\"learning_rate\": [0.01, 0.1], \"num_leaves\": [31, 50]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "best_params_list = []\n",
        "for i in range(n_repeats):\n",
        "    print(f\"\\n--- Repetition {i+1}/{n_repeats} ---\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100+i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "    for params in grid:\n",
        "        model = LGBMClassifier(**params)\n",
        "        model.fit(X_train, Y_train)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob)\n",
        "    })\n",
        "\n",
        "    best_params_list.append(best_params)\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "best_params_df = pd.DataFrame(best_params_list)\n",
        "print(summary)\n",
        "print(best_params_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti5LlIdNGG_Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"kernel\": [RBF()],\"max_iter_predict\": [100, 200]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "best_params_list = []\n",
        "\n",
        "for i in range(n_repeats):\n",
        "    print(f\"\\n--- Repetition {i+1}/{n_repeats} ---\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100+i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "\n",
        "    for params in grid:\n",
        "        gpc = GaussianProcessClassifier(**params)\n",
        "        gpc.fit(X_train, Y_train)\n",
        "        y_prob = gpc.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = gpc\n",
        "            best_params = params\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob)\n",
        "    })\n",
        "\n",
        "    best_params_list.append(best_params)\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "best_params_df = pd.DataFrame(best_params_list)\n",
        "print(summary)\n",
        "print(best_params_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZT-d_SCO8hM"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"n_estimators\": [100, 200],\"learning_rate\": [0.01, 0.1],\"max_depth\": [3, 5],\"subsample\": [0.8, 1.0]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "best_params_list = []\n",
        "\n",
        "for i in range(n_repeats):\n",
        "    print(f\"\\n--- Repetition {i+1}/{n_repeats} ---\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100+i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "    for params in grid:\n",
        "        gb = GradientBoostingClassifier(**params)\n",
        "        gb.fit(X_train, Y_train)\n",
        "        y_prob = gb.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = gb\n",
        "            best_params = params\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob)\n",
        "    })\n",
        "    best_params_list.append(best_params)\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "best_params_df = pd.DataFrame(best_params_list)\n",
        "print(summary)\n",
        "print(best_params_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"C\": [0.01, 0.1, 1, 10],\"penalty\": [\"l2\"],\"solver\": [\"lbfgs\"]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "for i in range(n_repeats):\n",
        "    print(f\"Running repetition {i+1}/{n_repeats} ...\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100 + i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    for params in grid:\n",
        "        lr = LogisticRegression(**params, max_iter=1000)\n",
        "        lr.fit(X_train, Y_train)\n",
        "        y_prob = lr.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = lr\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob),\n",
        "    })\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "print(summary)\n",
        "print(best_params_df)"
      ],
      "metadata": {
        "id": "4XjqNyo69a15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bnMRtZikFHY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "param_grid = {\"n_estimators\": [100, 200],\"max_depth\": [5, 10, -1],\"learning_rate\": [0.01, 0.1], \"num_leaves\": [31, 50]}\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "n_repeats = 10\n",
        "metrics_list = []\n",
        "best_params_list = []\n",
        "for i in range(n_repeats):\n",
        "    print(f\"\\n--- Repetition {i+1}/{n_repeats} ---\")\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100+i\n",
        "    )\n",
        "\n",
        "    best_auc = -1\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "    for params in grid:\n",
        "        model = LGBMClassifier(**params)\n",
        "        model.fit(X_train, Y_train)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(Y_test, y_prob)\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics_list.append({\n",
        "        \"Accuracy\": accuracy_score(Y_test, y_pred),\n",
        "        \"Precision\": precision_score(Y_test, y_pred),\n",
        "        \"Recall\": recall_score(Y_test, y_pred),\n",
        "        \"F1\": f1_score(Y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(Y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(Y_test, y_prob)\n",
        "    })\n",
        "\n",
        "    best_params_list.append(best_params)\n",
        "results_df = pd.DataFrame(metrics_list)\n",
        "summary = results_df.agg(['mean', 'std']).T\n",
        "best_params_df = pd.DataFrame(best_params_list)\n",
        "print(summary)\n",
        "print(best_params_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "cm_list = []\n",
        "for i in range(10):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=100+i\n",
        "    )\n",
        "\n",
        "    model = LGBMClassifier(learning_rate=0.1,max_depth=10,n_estimators=200,num_leaves=50)\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    cm = confusion_matrix(Y_test, y_pred)\n",
        "    cm_list.append(cm)\n",
        "cm_array = np.array(cm_list)\n",
        "cm_mean = cm_array.mean(axis=0)\n",
        "cm_df = pd.DataFrame(cm_mean,\n",
        "                     index=[\"Actual Inactive\", \"Actual Active\"],\n",
        "                     columns=[\"Predicted Inactive\", \"Predicted Active\"])\n",
        "cm_df.to_csv(\"LGBM_Confusion_Matrix.csv\")\n",
        "print(\"Mean Confusion Matrix Across 10 Repeats:\\n\", cm_mean)\n"
      ],
      "metadata": {
        "id": "IDeWSJ2SBsSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNDyKwjNB6In"
      },
      "outputs": [],
      "source": [
        "train = pd.concat([pd.DataFrame(X_train), Y_train.reset_index(drop=True)], axis=1)\n",
        "test = pd.concat([pd.DataFrame(X_test), Y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "train['model'] = \"Train\"\n",
        "test['model'] = \"Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsOgRPN9CEL7"
      },
      "outputs": [],
      "source": [
        "train['model'] = \"Train\"\n",
        "test['model'] = \"Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9R5si4ECI6i"
      },
      "outputs": [],
      "source": [
        "frames = [train,test]\n",
        "pca = pd.concat(frames)\n",
        "pca.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0l4qZUOBXQg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from pandas.plotting import scatter_matrix # Changed the import statement to use pandas.plotting\n",
        "# Plot the Figures Inline\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def PCA_plot(data):\n",
        "    # PCA's components graphed in 2D\n",
        "    # Apply Scaling\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from matplotlib import pyplot as plt\n",
        "    data_pca = data\n",
        "\n",
        "    # Apply Scaling\n",
        "    X = data_pca.drop('model', axis=1).values # Use .values instead of as_matrix()\n",
        "    y = data_pca['model'].values\n",
        "\n",
        "    # Formatting\n",
        "    target_names = ['Train','Test']\n",
        "    colors = ['#feb236', '#6b5b95']\n",
        "\n",
        "    # 2 Components PCA\n",
        "\n",
        "    fig=plt.figure(2, figsize=(8, 6)) # Adjust figure size if needed\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    X_std = StandardScaler().fit_transform(X)\n",
        "    X_r = pca.fit_transform(X_std)\n",
        "\n",
        "    for color, i, target_name in zip(colors, ['Train','Test'], target_names):\n",
        "        plt.scatter(X_r[y == i, 0], X_r[y == i, 1],\n",
        "                    color=color,\n",
        "\n",
        "                    label=target_name)\n",
        "    plt.xlabel('PC1',  fontstyle= \"normal\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('PC2',  fontstyle= \"normal\", fontsize=14, fontweight='bold')\n",
        "\n",
        "\n",
        "    plt.grid(False) #remove grid in 2D plot\n",
        "\n",
        "    # Add outline around axes\n",
        "    ax = plt.gca()\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_linewidth(1)\n",
        "        spine.set_color('black')\n",
        "\n",
        "    # Legend with box and black outline\n",
        "    legend = plt.legend(loc='best', scatterpoints=1)\n",
        "    legend.get_frame().set_linewidth(2)\n",
        "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.tick_params(direction=\"out\", labelsize=12)\n",
        "    plt.savefig('applicability_domain.svg')\n",
        "PCA_plot(pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdPyQYjeg90D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming 'X_train', 'Y_train', 'X_test', 'Y_test' are your training and test data\n",
        "\n",
        "# Train your best model (Extra Trees, for example)\n",
        "best_model = LGBMClassifier()\n",
        "best_model.fit(X_train, Y_train)\n",
        "\n",
        "# Applying PCA to training data\n",
        "pca_train = PCA(n_components=2)\n",
        "X_train_pca = pca_train.fit_transform(X_train)\n",
        "\n",
        "# Applying PCA to test data\n",
        "pca_test = PCA(n_components=2)\n",
        "X_test_pca = pca_test.fit_transform(X_test)\n",
        "\n",
        "# Visualize PCA of training data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], color='blue', label='Training-set')\n",
        "\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "# Overlay PCA of test data on the same plot\n",
        "plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], color='red', label='Test-set')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('PCA.svg')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Import seaborn\n",
        "\n",
        "# Assuming df_final is your DataFrame\n",
        "features = ['MW', 'LogP', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', 'TPSA']\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_final[features]\n",
        "y = df_final['bioactivity_class']\n",
        "\n",
        "# Standardize the data (important for PCA)\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)  # You can change the number of components as needed\n",
        "principal_components = pca.fit_transform(X_std)\n",
        "\n",
        "# Create a new DataFrame with the principal components and relevant columns\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "# Ensure the 'bioactivity_class' and 'pIC50' columns are also in pca_df\n",
        "pca_df['bioactivity_class'] = df_final['bioactivity_class'].values\n",
        "pca_df['pIC50'] = df_final['pIC50'].values\n",
        "\n",
        "# Diagnostic print statement (optional)\n",
        "print(\"Unique values in 'bioactivity_class' column:\", pca_df['bioactivity_class'].unique())\n",
        "\n",
        "# Plot the PCA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Define the colors for each bioactivity class\n",
        "colors = {'active': '#6b5b95', 'inactive': '#feb236'}\n",
        "\n",
        "for bioactivity_class, color in colors.items():\n",
        "    # Filter the data for the current bioactivity class\n",
        "    class_data = pca_df[pca_df['bioactivity_class'] == bioactivity_class]\n",
        "    # Plot the data for this class with the specified color\n",
        "    if not class_data.empty:\n",
        "        plt.scatter(class_data['PC1'], class_data['PC2'], alpha=0.4, color=color, label=bioactivity_class, edgecolor='black')\n",
        "    else:\n",
        "        print(f\"Warning: No data found for bioactivity_class '{bioactivity_class}' to plot.\")\n",
        "\n",
        "\n",
        "plt.xlabel('PC1', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('PC2', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "\n",
        "# --- Add this line to save the figure before showing it ---\n",
        "plt.savefig('PCA.svg', format='svg')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8E_M-3zOpyQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOwVWGhEnZSz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats\n",
        "\n",
        "# Assuming 'df_final' is your DataFrame containing the relevant columns\n",
        "\n",
        "# List of columns\n",
        "columns_of_interest = [\"MW\", \"LogP\", \"NumHAcceptors\", \"NumHDonors\", \"NumRotatableBonds\", \"TPSA\"]\n",
        "\n",
        "# Dictionary to store results\n",
        "summary_statistics = {}\n",
        "\n",
        "# Separate data for group1 and group2\n",
        "for group_name in df_final['bioactivity_class'].unique():\n",
        "    summary_df = df_final[df_final['bioactivity_class'] == group_name][columns_of_interest].describe().transpose()\n",
        "\n",
        "    # Calculate skewness and kurtosis\n",
        "    skewness_values = df_final[df_final['bioactivity_class'] == group_name][columns_of_interest].skew()\n",
        "    kurtosis_values = df_final[df_final['bioactivity_class'] == group_name][columns_of_interest].kurt()\n",
        "\n",
        "    # Add skewness and kurtosis to the summary DataFrame\n",
        "    summary_df['skew'] = skewness_values\n",
        "    summary_df['kurt'] = kurtosis_values\n",
        "\n",
        "    # Add p-values to the table\n",
        "    p_values = []\n",
        "    for column in columns_of_interest:\n",
        "        _, p_value = scipy.stats.ttest_ind(\n",
        "            df_final[df_final['bioactivity_class'] == 'active'][column].dropna(),\n",
        "            df_final[df_final['bioactivity_class'] == 'inactive'][column].dropna()\n",
        "        )\n",
        "        p_values.append(p_value)\n",
        "\n",
        "    # Append p-values to the summary DataFrame\n",
        "    summary_df['p-value'] = p_values\n",
        "\n",
        "    # Store the summary DataFrame in the dictionary\n",
        "    summary_statistics[group_name] = summary_df\n",
        "\n",
        "# Display the results\n",
        "for group_name, stats_df in summary_statistics.items():\n",
        "    print(f\"\\nSummary statistics for {group_name}:\")\n",
        "    print(stats_df[['min', 'max', '50%', 'mean', 'skew', 'kurt', 'p-value']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}